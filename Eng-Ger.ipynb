{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05b7814-38ba-4d3f-bcc9-4adab02ef2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad3bf0c-ce35-4ef7-b657-de7ade8c69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"eng-ger.csv\")\n",
    "english_sentences=data[\"ENGLISH\"].tolist()\n",
    "german_sentences=data[\"GERMAN\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80885643-19df-4b4c-845e-64d69b5c03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize English sentences\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(english_sentences)\n",
    "total_words_eng = len(tokenizer_eng.word_index) + 1\n",
    "\n",
    "# Tokenize German sentences\n",
    "tokenizer_ger = Tokenizer()\n",
    "tokenizer_ger.fit_on_texts(german_sentences)\n",
    "total_words_ger = len(tokenizer_ger.word_index) + 1\n",
    "\n",
    "# Convert sentences to sequences\n",
    "input_sequences = tokenizer_eng.texts_to_sequences(english_sentences)\n",
    "output_sequences = tokenizer_ger.texts_to_sequences(german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd11ac4-14c8-4d61-9d81-af25ca8539f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenizer_eng.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_eng.to_json(), f)\n",
    "\n",
    "with open(\"tokenizer_ger.json\", \"w\") as f:\n",
    "    json.dump(tokenizer_ger.to_json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e320a0-d0ff-44fc-b1eb-7a3f4eeb97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have the same length\n",
    "max_len = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in output_sequences))\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='post')\n",
    "output_sequences = pad_sequences(output_sequences, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5acb82-ac27-4553-a5db-7e65d79ab566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create input and output for the model\n",
    "X = np.array(input_sequences)\n",
    "y = np.array(output_sequences)\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "# y_one_hot = to_categorical(y, num_classes=total_words_ger)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179fed38-ab0c-48b3-b918-bbbdbeb4346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words_eng, 128, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(total_words_ger, activation='softmax'))\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical_crossentropy loss\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30b8049-735f-456b-ad13-7e5b16809684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2866/2866 [==============================] - 3469s 1s/step - loss: 0.8332 - accuracy: 0.8967 - val_loss: 0.5604 - val_accuracy: 0.9134\n",
      "Epoch 2/5\n",
      "2866/2866 [==============================] - 3499s 1s/step - loss: 0.4956 - accuracy: 0.9167 - val_loss: 0.4850 - val_accuracy: 0.9183\n",
      "Epoch 3/5\n",
      "2866/2866 [==============================] - 3575s 1s/step - loss: 0.4127 - accuracy: 0.9218 - val_loss: 0.4594 - val_accuracy: 0.9199\n",
      "Epoch 4/5\n",
      "2866/2866 [==============================] - 3366s 1s/step - loss: 0.3591 - accuracy: 0.9263 - val_loss: 0.4440 - val_accuracy: 0.9218\n",
      "Epoch 5/5\n",
      "2866/2866 [==============================] - 3231s 1s/step - loss: 0.3225 - accuracy: 0.9303 - val_loss: 0.4388 - val_accuracy: 0.9226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20f9cce93d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with callbacks\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f37af279-731b-459c-9385-340e74819c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911/1911 [==============================] - 648s 339ms/step - loss: 0.4388 - accuracy: 0.9226\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad6e329-08b1-494a-8506-8909312bab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"language_translation_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5f448f-34fc-4b13-b1c3-8475959b3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"max_len.txt\", \"w\") as f:\n",
    "    f.write(str(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efed7c93-0e6e-408f-a0fc-4761f44d1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "loaded_model = load_model(\"language_translation_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1f3e611-a5ca-491e-9ac4-89d67227381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an English sentence:  hi \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter an English sentence: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize and pad the input sequence\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_eng\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences([user_input])\n\u001b[0;32m      6\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m pad_sequences(input_seq, maxlen\u001b[38;5;241m=\u001b[39mmax_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Predict the output sequence\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_eng' is not defined"
     ]
    }
   ],
   "source": [
    "# Take input from the user\n",
    "user_input = input(\"Enter an English sentence: \")\n",
    "\n",
    "# Tokenize and pad the input sequence\n",
    "input_seq = tokenizer_eng.texts_to_sequences([user_input])\n",
    "input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Predict the output sequence\n",
    "predicted_seq = loaded_model.predict(input_seq)\n",
    "\n",
    "predicted_text = []\n",
    "for word_index in np.argmax(predicted_seq, axis=-1)[0]:\n",
    "    if word_index != 0:  # Ignore padding index\n",
    "        word = tokenizer_ger.index_word.get(word_index, '<OOV>')\n",
    "        predicted_text.append(word)\n",
    "\n",
    "# Display the result\n",
    "german_translation = ' '.join(predicted_text)\n",
    "print(f\"German Translation: {german_translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38ce1a-8d27-4abf-bdb4-dd99da19d4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
